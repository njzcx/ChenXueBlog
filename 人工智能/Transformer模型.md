1. 了解transformer诞生故事 -> 抖音视频介绍

总结：transformer最初是由谷歌的翻译团队为解决机器翻译问题而开发的

1. 课前复习 https://zhuanlan.zhihu.com/p/28054589

1. CNN、RNN、LSTM
2. Embedding、Word2Vec
3. Attention

## RNN

RNN（Recurrent Neural Networks）循环神经网络，是一种专门用于处理序列数据的神经网络类型。序列数据在生活中很常见，如时间序列（股票价格随时间的变化）、文本序列（句子中的单词顺序）、语音序列等。与传统的神经网络不同，RNN 具有循环结构，能够对序列中的信息进行 “记忆” 和处理。

#### RNN原理

提取输入序列(x1,x2...xn)中每个元素的特征(h1,h2...hn)，特征为隐藏层，特征提取依赖于当前元素以及前一个元素的特征，如h1=f(Ux1+Wh0+b)，最后对所有特征计算得到输出，如y1=Soft max(Vh1+c)

#### RNN特征和场景

- 输入输出序列必须等长
- 适用于计算视频中每一帧的分类标签。因为要对每一帧进行计算，因此输入和输出序列等长。还有输入为字符，输出为下一个字符的概率。这就是著名的Char RNN。

## Transformer

Transformer 是一种在自然语言处理（NLP）和其他序列数据处理任务中广泛使用的深度学习架构。以下是快速学习 Transformer 的关键要点：



**一、架构概述**

1. **核心组件**

- **多头注意力机制（Multi - Head Attention）**

- 这是 Transformer 的关键创新之一。它允许模型同时关注输入序列的不同部分。例如，在处理句子 “我喜欢在美丽的公园里跑步” 时，多头注意力机制可以同时关注 “我” 与 “跑步” 的关系，“美丽的公园” 这一短语内部的关系等。
- 计算过程涉及查询（Query）、键（Key）和值（Value）。对于每个头（head），通过计算 Query 和 Key 的相似度得分，来确定对 Value 的关注程度。然后将多个头的结果拼接并线性变换，得到最终的输出。

- **前馈神经网络（Feed - Forward Network）**

- 这是一个简单的全连接神经网络，它在每个位置独立地对多头注意力机制的输出进行处理。它通常由两个线性层和一个激活函数（如 ReLU）组成。其作用是对序列中的每个元素进行进一步的特征提取和转换。

1. **层结构**

- Transformer 由多个编码器（Encoder）和解码器（Decoder）层堆叠而成。

- **编码器层**：主要包括多头注意力机制和前馈神经网络，还有一些层归一化（Layer Normalization）操作。它的任务是对输入序列进行编码，将输入的语义信息转换为一种适合机器理解的向量表示。

- **解码器层**：除了多头注意力机制和前馈神经网络外，还有一个额外的多头注意力机制，用于关注编码器的输出。这个机制帮助解码器在生成输出序列时，结合输入序列的信息，从而生成更准确的结果。

  

**二、输入输出表示**

1. **输入编码**

- 对于文本等序列数据，首先要将单词或符号转换为向量表示。这通常使用词嵌入（Word Embedding）技术，将每个单词映射到一个固定维度的向量空间。例如，在一个简单的情感分析任务中，单词 “快乐” 可能被映射到一个向量 [0.2, 0.3, -0.1,...]。
- 同时，为了让模型能够理解单词的位置信息，还会添加位置编码（Position Encoding）。位置编码是一种能够表示单词在序列中位置的向量，它可以是固定的（如正弦 - 余弦函数生成的编码），也可以是通过学习得到的。

1. **输出生成**

- 在解码器输出层，根据任务的不同会有不同的输出形式。例如，在机器翻译任务中，输出是目标语言的单词概率分布。模型通过对这个概率分布进行采样或选择概率最高的单词，来生成翻译后的句子。在分类任务中，输出可能是类别概率，通过选择概率最高的类别作为预测结果。

  

**三、训练过程**

1. **损失函数选择**

- 根据任务类型选择合适的损失函数。对于分类任务，常用交叉熵损失（Cross - Entropy Loss）；对于生成任务，如机器翻译，可以使用负对数似然损失（Negative Log - Likelihood Loss）。这些损失函数衡量了模型预测结果与真实结果之间的差异。

1. **优化器使用**

- 常用的优化器有 Adam 等。优化器会根据损失函数的梯度来更新模型的参数，以减小损失，使模型的预测结果更接近真实结果。在训练 Transformer 时，通常需要经过多个轮次（epochs）的训练，不断调整参数，直到模型收敛，即损失不再显著下降。



**四、应用场景和示例**

1. **机器翻译**

- Transformer 架构在机器翻译任务中取得了巨大的成功。例如，谷歌翻译等工具使用 Transformer 来将一种语言的句子转换为另一种语言的句子。它能够很好地处理长序列，并且能够捕捉句子中单词之间的复杂语义关系，从而生成更准确、更自然的翻译结果。

1. **文本生成**

- 在故事生成、摘要生成等任务中也有广泛应用。例如，给定一个新闻标题，Transformer 可以生成新闻摘要。它通过对输入标题的编码，然后在解码器中逐步生成摘要内容，利用多头注意力机制关注标题中的关键信息，生成连贯的摘要。