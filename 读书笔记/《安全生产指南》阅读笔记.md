## 面向失败设计 理论篇	

### 一、面向失败设计概述

​	故障注定会发生，随着时间的流逝一切软硬件运行终将失败，而保障业务持续可用，取决于我们愿意投入多少精力和成本，沉淀多少能力，是否预料到不可知的情况。

​	因为失败无处不在，尤其当系统架构复杂性增高，随之而来的失败概率也提高了。

> 一个优秀的架构师，通常是一个悲观主义、现实主义者。

**失败原因可能有**：

- 硬件问题：硬件会老化、会被外力损坏、有残次品率
- 软件bug：没有人可以保证程序没有bug，再者现在升级迭代比较快，bug不可避免
- 配置变更错误：运维变更考虑不全而疏漏
- 系统恶化：长时间工作的程序不再可用。如自增变量越界，缓存、磁盘、数据库空间不足
- 超预期流量：双十一或健康码
- 外部攻击：流量或安全攻击。如：DDOS和CC攻击
- 依赖库问题：二方库、三方库可能存在漏洞
- 依赖服务问题：其他服务可能不可用、超时，这种失败导致自己服务不可能，导致持续辐射，变成大面积服务不可用。

**如何应对上述失败**：

面向失败设计，将失败当成系统设计的一部分，**能够从失败中快速恢复时面向失败的核心思想**。

- 冗余设计避免单点故障：集群多节点部署，数据库主备，存储多副本。
- 宏观多活架构：预防天灾人祸，冷备、热备、两地三中心、多活。
- 服务能力与依赖的自我保护：通过优良的设计，在任何系统破坏后，核心功能仍能够保持处于正常工作状态。如限流、降级等。
- 不可预料场景的预案：不是所有失败都可以自动恢复的，需要针对未知失败做好预案。
- 精细化监控体系：感知失败发生。监控告警、根因定位、智能预测、智能决策。
- 自动化运维管控：白屏运维，避免操作失误、决策失误。运维需要遵循灰度原则，降低失败的爆炸半径。
- 失败演练：面向失败设计做实践验证。

### 二、冗余设计避免单点故障

要解决单点故障，需要**通过冗余和隔离**实现。如：多机房、电力双线、双线网线网卡、磁盘冗余、主备数据库、集群多节点。

- 冗余可快速接入并承担已发生的故障。

- 隔离可使故障不影响现有系统的服务质量。

针对集群多节点，无状态节点扩容即可，有状态节点需要考虑：

- 状态同步：流量分配规则，如一致性Hash算法；或共享存储系统。
- 操作互斥：分布式锁；分区分割模式。
- 主节点冗余：主备部署；分布式选主。

站点架构一般采用分层解决单点故障。

单点故障的故障域局限于机房内的服务节点，当故障域扩大到机房、地域、光纤，则进入到容灾架构的领域中了。

### 三、宏观多活架构

容灾系统评估标准Share78：https://zhuanlan.zhihu.com/p/66835461

容灾系统的设计，主要受制于建设成本，分为7级。

**本章主要讲 集团多活容灾架构的思路和设计，其中包含优点和局限性。可作为竞品分析。**

### 四、服务能力与依赖的自我保护

分布式架构有很多依赖，为避免局部问题被放大成全局问题，需要以最小的粒度将失败隔离开，以保证系统稳定性。

- 尽早拦截无效请求：DDos清洗拦截同个IP同个用户的大量请求；对业务不产生实际意义的请求，如秒杀。
- 漏斗原则：越到链路后端，耗费资源越多，所以从前到后的请求量应逐级递减。如通过CDN、缓存手段。
- 隔离原则：1、下游弱依赖降级；2、下游强依赖隔离，通过线程池、信号量、并发数实现；3、异常机器可隔离；4、异常流量可隔离
- 流量错峰：匀速器来削峰填谷。
- 按需分配：provider通过QPS限流来提供服务，此外需要提供限流后的Fallback。
- 其他：失败请求用户体验；失败请求是否可以幂等

### 五、为失败准备预案

- 什么是预案
  - 预案是“知识点”和“经验教训”，在非系统预期的场景下，系统无法自己处理时的一些人工处置方案。
- 为什么要准备预案
  - 有些系统问题并没有规律可循，或没有自动化处理方式。但通过预案，可在关键时刻，将处理这些问题
  - 有前提条件需要预案：比如系统支持100个并发，那超过100个并发怎么办？
  - 需要人工干涉的步骤需要预案：确保操作正确性。
  - 故障场景需要预案
  - 业务变更需要预案
- 如何准备预案
  - 预案执行前的判断条件要清晰
  - 预案执行步骤要准确、清晰
  - 预案执行后要有校验步骤
  - 预案执行的各参与角色尽量在流程上隔离，保证每个角色至关心自己范围内的事情。
  - 预案的生命周期：梳理预案（明确需要预案的风险点）、开发预案（针对风险点设计预案）、测试预案、演练预案（定期演练保证预案的准确性，预案的准确性应该是最高的）、下线预案。

### 六、精细化监控体系

监控的三个目的：

- 故障检测：检测故障
- 性能分析：检测性能瓶颈
- 容量规划：分析容量设计是否合理

监控视角：

- 白盒监控：站在系统视角，回答“哪里出了问题”，协助定位根因。
- 黑盒：站在用户视角，回答“系统出了问题吗”，支持检测和告警。

监控类别：

- 全栈监控：包含下列所有的监控类别
- 业务监控：从业务上监控服务状况，回答“系统出了问题吗”。
- 应用监控：从应用上监控QPS、RT、成功率失败率、JVM等核心指标，回答“哪里出了问题”。
- 容器监控：从容器上监控CPU、内存、磁盘IO和容量
- 主机监控：从主机上监控CPU、内存、磁盘IO和容量、网卡、网络联通性等。

监控技术：

- Metrics：度量，收集一段时间的数据，聚合特征并以某种模型展示出来。开源工具有dropwizard、Iris、Prometheus。
- Tracing：全链路监控，基于Google Dapper论文，可在分布式架构下，实现点到点每一条链路上的监控。核心技术是通过Agent代理插桩埋点，记录TraceId和TraceLog到KV存储设备，之后统计展示。开源工具有zipkin、openTracing、jaeger。
- Logging：监控日志并采集、清洗、展示。一般基于ELK实现。

监控维度：需要站在全局架构视角去设计，例如从上到下 站点 - 机房 - 集群 - 实例 4个基本维度。

监控精度：秒、分钟级别。分别在实效性和成本上有不同的侧重。

统计方式：采用合理的统计方式，避免引入噪音，影响决策。常用的统计方式有原始值、最大/小值、总和、平均值、数量、百分比、百分位等。

智能化监控：传统的人工设置阈值没办法应对大规模分布式应用的监控要求。而通过统计学和机器算法在不同场景下使用不同的算法，实现监控智能化，是未来的发展趋势。如线性回归、加权移动平均、指数平滑算法、聚类算法、深度学习模型等。

### 七、自动化运维管控

运维体系演进基本遵循：人肉时代 -> 工具时代 -> 平台/系统时代 -> 数据智能时代 

自动化体系演进：操作自动化 -> 场景自动化（结合上下文和外部环境） -> 数据驱动自动化（基于采集数据和算法模型）

运维常见日常5个问题：人工配置规则失误；灰度升级与回滚；消息队列丢数感知与补偿；同主机中VM相互干扰导致的CPU飙升；告警下如何熔断止血

对应解决方案（5个特征）：

- 监控指标精细透明：分为依赖基础设施指标和业务指标，前者如宿主机、中间件、存储、数据库等，后者如成交率、退款率等
- 问题快速发现
- 预案自动执行
- 变更安全可控
- 答疑高效支持

自动化运维平台设计核心思路：把一切变更标准化、流程化、自助化、自动化，降低人工干预程度，提升操作效率。一切以客户需求出发，持续抽象和迭代。

运维平台流程：监控数据采集 -> 异常识别 -> 根因分析 -> 异常/变更处理 -> 安全机制(灰度、回滚、权限控制)

运维管理平台本质上是解决安全和效率问题，衡量运维平台的好坏，可以通过：

- 故障后问题的定位、修复时间是否缩减。
- 日常答疑支持上的投入程度是否降低
- 开发人员工作打断频率是否下降，在重复性事务上的投入比重是否减少，关键产出是否提升。

### 八、失败演练

故障场景下的问题：

- 故障预案是否真实有效？
- 故障恢复工具是否实现了容灾？
- 处理故障人员是否熟练？
- 沟通机制是否疏漏？
- 容灾措施影响是否会辐射上一层？

故障演练定义：通过平台化方式沉淀故障场景，以可控成本在线上故障重放，通过持续性的演练和回归方式暴露问题，提升问题的响应与修复能力，缩短故障恢复时长。

故障演练的工具和策略：

- 引入混沌工程，建立面向失败设计和拥抱失败的文化，通过实验变量去暴露问题。
- 实施混沌工程，定义一个清晰可衡量的目标。如故障覆盖率，SLA等
- 推广混沌工程，贴近生产环境实验，结果越真实。准备大于执行，重点关注实验方案的评估，明确系统稳态和终止条件。规模化推广。

实践策略：

- 建立一个围绕稳态行为的假说：从有预期的故障注入和无预期的突袭演练，观察可测量的输出和短时间的测量结果，验证系统是否工作。
- 多样化真实世界的事件：从网络、存储、虚拟化VM、操作系统、中间件、数据库、负载均衡、应用等整体考虑故障的完备性。
- 在生产环境运行实验：仿真环境、沙箱环境，在生产环境控制好爆炸半径进行演练更容易暴露问题。
- 持续自动化运行实验：一整套演练流程包含：计划、执行、观察、记录、还原、分析。能够自动化编排和持续运行是基础。
- 最小爆炸半径：注入隔离、流量隔离、数据隔离。

## 面向失败设计 实践案例篇

### 一、架构域

#### 1、单点故障

- 无论系统其他部分做得再完备，只要系统中存在单点问题并发生故障，将引起整体故障。
- 遵循高可用架构设计，做好软硬件冗余架构，在故障出现时能够切换。
- 定期对单点进行巡检、迭代优化，通过故障注入发现系统中的潜在单点故障。要有系统化思考能力，即见树木又见森林。

#### 2、负载不均

- 负载不均会使某个节点长期处于过载状态，当流量到临界点或突增，可能导致集群服务节点全部打爆导致不可用（葫芦娃救爷爷，一个一个送）；存储热点也属于负载不均的一种表现。
- 观察负载是否均衡，不均衡时需要水平扩容或重新设计分配方案。
- 热点数据问题可以通过分桶或热点缓存来规避，尽量把请求分散到多个节点上。
- 要注意负载均衡设备自身是否会成为瓶颈

#### 3、有状态

- 无状态的单次请求不依赖其他请求，服务器本身不存储任何信息，比如HTTP协议。有状态会依赖其他请求或缓存信息，比如Session，有状态服务由于状态绑定比较难扩展。
- 在分布式设计中，考虑到扩展性，应尽量规避有状态设计。
- 不要将IP和磁盘路径写死在代码或配置中。
- 本地磁盘只写日志，不存储业务相关数据。

#### 4、事后监控

- “为什么我们没有提早发现问题”，这说明监控项是有遗漏的，对应的Action是补监控。但实际上在系统设计时就应该考虑监控埋点，设计监控项来回答“出问题了吗”、“哪里出问题了”、“具体什么问题”。
- 系统设计之初就应该设计好监控，而不是事后再补。
- 监控覆盖指标可分为OS级别（CPU、Mem、Load、磁盘、网络）、应用容器级别（Nginx、Jvm、Tomcat指标）、业务级别（QPS、TPS、RT、成功率、转化率等）。
- 指标要有环比视图，和告警能力。

#### 5、不可回滚

- 任何升级或变更都会带来系统性风险，所以应该具备回滚能力。
- 变更和回滚尽量做的自动化，当回滚不便做自动化时，可以通过“回滚脚本”或“回滚方案”支持。
- 不要盲目自信，升级变更一定要有回滚方案，而且一定要在上线前review。

#### 6、不可降级

- 当系统访问量激增、性能下降，非核心服务影响到核心服务的使用时，为保证主流程可用，需要对非核心服务进行降级。核心思想是通过对非核心界面和服务有策略不做处理，来释放服务器资源保证核心服务正常运作。
- 常见的降级思路：1）通过开关统一降级；2）多个开关组合形成预案，按预案降级；3）限流；4）熔断；
- 系统设计时要甄别强弱依赖，弱依赖不可用时不能导致系统崩溃；开关、预案需要沉淀下来，并适时做演练。

#### 7、缺乏隔离

- 当系统出现故障时，隔离能够将故障的影响控制在一定范围，限制爆炸半径。另一方面也可以减少资源竞争。
- 常见隔离方案：
  - 动静资源隔离（CDN+Server）
  - 线程隔离：不同重要性任务放在不同线程池中
  - 进程隔离：应用拆分成多个重要性不同的服务，分开部署
  - 集群隔离：将同一个服务部署多个集群，集群分组提供不同的SLA
  - 机房隔离：服务多机房部署，机房内优先调用
  - 租户隔离：租户之间隔离，互相不影响。
- 系统设计时要考虑好核心服务和非核心服务，从上述隔离方案中逐层对服务拆分，保证其隔离性，承诺不同的SLA。

#### 8、缺乏冗余备份

- 工程领域、航天领域、计算机领域都有应用“冗余备份”模型，对核心系统的依赖做多份冗余。
- 是否要做冗余备份，要结合可用性以及成本来看是否值得。
- 对应用外部依赖进行整理，依赖最小化，重要依赖组件增加冗余备份机制。

### 二、设计域

- 良好的设计需要趋同大家共同的理解，如有特殊性，需要在代码里做相关说明，避免很多低级故障，避免给同事挖坑。
- 设计师需要具备事物抽象能力，解耦能力；有需要从细节把控，做到大处着眼，小处着手。
- 软件设计领域有很多设计思想：面向过程、面向对象、面向服务、面向消息、面向失败，都是在不同阶段解决不同问题，没有好坏区分。
- 编码设计：命名要见名知意；注意解偶，特别是开闭原则
- 分布式设计：关注一致性，避免脑裂问题。
- 缓存设计：观察命中率，避免无效查询，打穿缓存。
- 容量设计：明确容量上限，做好保护措施。
- 依赖设计：明确强弱依赖，做好弱依赖旁路化和降级和强依赖冗余
- 日志设计：日志异步输出，避免无效日志。日志占用空间可控。
- 监控设计：确定并监控关键指标，具备报警功能，但又不要过多无效报警。
- 权限设计：够用原则，避免大权限、大账号。
- 表结构设计：分库分表关注热点数据问题；关注表数据量大小

#### 1、滥用日志策略

- 建议采用异步日志机制。日志一般存储在本地磁盘，采用异步日志，可保证业务流程不受磁盘故障影响，让写日志在旁路工作。
- 采用异步刷盘策略。将日志放入队列中，由刷盘线程定期处理，设计较为宽松的丢弃策略，避免高频日志频繁刷盘，使IO成为瓶颈，进而造成日志堆积影响内存和GC。
- 以固定文件大小保存淘汰文件，保护磁盘容量。

#### 2、缺乏自我保护

- 任何系统都有处理能力上限，所以系统自我保护很重要，系统调用其他系统时，要认为它是不靠谱的，设计一套自己的容错方案做依赖管理。
- 例如：突发的大并发请求服务端；发生异常大量的错误堆栈打印，加剧写磁盘压力。
- 通过TCP滑动窗口或拥塞控制机制缓解；通过限流组件预防突发流量；对不稳定的或超时的依赖调用自动熔断；限流后的处理逻辑尽量简单快速，避免二次伤害。

#### 3、缓存设计不当

- 缓存和数据库不一致；命不中数据导致击穿；热点数据导致雪崩。
- 通过缓存空值key或比较大的布隆过滤器，避免缓存击穿。
- 通过一致性hash进行热点散列。
- 缓存层只在数据层和服务层之间，避免嵌套调用。
- 调用接口异常，错误数据不放入缓存。
- 恰到好处的缓存策略。

#### 4、容量评估不准

- 设计系统时，需要根据压测数据，设置资源值和流控阈值，包括不限于流控QPS、连接池、上下游关联服务性能。
- 提升性能敏感性。做任何变更前，要清楚会不会带来性能影响。

#### 5、耦合过重

- 高耦合带来的问题是牵一发而动全身，不易维护、演进。
- 单个应用设计，要考虑单模块失效是否会影响整体服务。可以从代码层面、进程层面、机器、应用层面解偶，降低整体耦合。

#### 6、滥用同步

- 同步、异步没有优劣之分，要看应用在什么样的实际场景。同步优势在于开发和调试简单，交互即时反馈，但链路过长，可用性会下降。异步优势在于解偶依赖，降低链路故障的影响，但代码和调试会有一定的复杂性。
- 系统设计中，系统间交互、系统内模块交互，尽量采用异步通信。
- 独立任务、耗时任务，应该采用异步，减少阻塞和串联失败的可能性。
- 大流量服务打印日志，建议采用过AsyncAppender实现，并配合合理的buffer以及丢弃策略。

#### 7、非幂等

- 幂等：同一操作多次请求，结果必须是一致的，**不会因为多次点击或提交而产生副作用**。例如订单已支付，服务处理成功，但返回客户端失败了，再次支付应该不重复扣款。
- 同一次事务多次重复操作，确保操作唯一性和事务结果唯一性，防止重复提交。
- 同一任务多次重试，确保重试对结果无影响。

#### 8、敏感信息泄漏

- 客户信息展示注意权限和脱敏
- 日志、存储的敏感数据通过二方库脱敏

#### 9、对失败考虑不充分

- 大部分故障都是由于基础设施、三方依赖、软负载、配置中心、中间件异常导致的。
- 系统调用其他服务时，要认为他们可能会失败，而设计一套自己的方案，避免失败影响核心业务。包括不限于三方依赖容灾设计、服务熔断设计、单Region高可用、两地三中心。

#### 10、数据库索引不合理

- 唯一索引中，字段如果为NULL，那么该唯一性将会被打破（BDB引擎除外）。
- 建议：
  - 无论表大小，都建议加上索引，避免全表扫描。优先原则：单索引 > 联合索引，窄索引 > 宽索引
  - 选择区分度高的字段建立索引
  - 多字段and或多表join，注意满足最左前缀原则，区分度高的字段放在前面。
  - 尽量使用覆盖索引、前缀索引【不知道啥意思】
  - order/group by则需要排列方向一致，才能利用索引。
  - 唯一索引的字段要有非空限制或默认值。
  - 索引调整一定要确保drop+add是通过一条alter执行的

#### 11、数据库表结构设计不合理

- 主键设计要避免varchar类型，因为会加大索引负担，影响BP命中率、查询效率、插入效率。建议采用bigint类型的自增字段。
- 分库分表建议采用区分度高的唯一键，避免写入数据倾斜，查询时广播到所有节点的问题。

#### 12、误用数据库limit查询

- limit一般用于翻页查询、批量导数据控制批次。问题是：limit 1000,10会先读取1010行再扔掉前1000行，效率差，如果没有索引那么效率更差。
- 解决办法：limit和order by组合，将排序字段加入索引列；用主键代替限定查询范围，例如select id,xx from table where id > min(id) and id < max(id) and order by id limit 50，取最大的id作为下次查询的minId，不断重复查询。

#### 13、小表随意truncate

- truncate和delete table都可以清空表。
  - delete操作会记录每行的redo、undo、binlog日志，且执行后表空间不释放，只是page为空页；
  - truncate是drop table + create table的组合，会释放表空间，事务日志只记录页释放，所以数据无法恢复。
- truncate性能优于delete，但truncate导致数据库抖动，可能造成数据库暂时不可用状态，每次使用truncate需要sleep一段时间，要小心使用。具体原因与buffer pool相关，删除表时会对buffer pool加全局锁。

#### 14、一次性批量delete数据

- delete大量数据，会导致导致数据库性能、稳定性变差；update同理。
  - 产生大量binlog，master-slave同步延迟会增高，当master出现故障发生切换，会导致切换延迟大，且数据有损量也大。
  - delete数据量太大，undo log无法很快被purge，影响新会话的select查询效率。
- 建议delete、update操作按批次拆分执行，每个批次执行一小部分数据，批次间sleep进行等待，降低对数据库的影响。

#### 15、对异常状态进行兜底设计

- 平台应该信任并忠于用户的请求，但人都会犯错，对于一些明显异常的用户操作，不加阻拦很容易出现重大故障。例如删除生产环境上的所有的LVS VIP。
- 核心系统上要设计一些极端情况的兜底逻辑，不给用户犯错的机会。

#### 16、对重要数据备份

- 数据备份时所有系统都要做的一件事，当出现重大故障时可以快速恢复。例如用户删除生产环境上的所有的LVS VIP，如果有快照备份数据，就可以在最短时间恢复。

### 三、发布域

安全发布三板斧：可灰度，可观测，可回滚

发布策略：

- 金丝雀发布：用单台或5%以内的机器先发布，验证正常后，在整体发布。缺点是一般通过手工验证，大规模发布的问题无法发现。
- 滚动发布：金丝雀的改进，多个批次逐步增加发布数量，每个批次都要观察。缺点是发布周期会比较长，高一致性场景不适用。
- 蓝绿发布：部署两套新旧版本同时提供服务，通过切流实现发布。需要额外冗余的机器资源支持。

#### 1、无灰度流程

- 灰度发布是发布系统不可或缺的一部分。
- 灰度最佳要求是一个环境隔离，独立监控，流量可控的安全环境，也是一个可小量变更的环境，在该环境中通过小流量独立验证。

#### 2、错误灰度方案

- 灰度发布本身要考虑是否对配置项结构、数据库表模型、API变更向前兼容和新旧版本兼容。
- 如果不能做到兼容，应确保灰度流量不要逃逸到正常机器上；写数据通过加version保证新版本的数据只有新逻辑处理。
- 灰度方案要具备可逆性，即可以增加发布比例，也可以减少发布比例。

#### 3、未经测试上线

- 主观上常见问题：觉得改动不大，研发直接绕过测试发布；搭车发布代码。
- 客观上常见问题：回归测试范围不广泛；自动化测试用例不足；回归测试不充分；只运行低级别的回归测试；只执行变更相关的回归测试。
- 最佳实践：
  - 严格发布流程，设置卡点，必须经过测试同意才能发布。
  - 禁止搭车发布代码，每一个分支要有owner对这个分支上的修改负责，并能给出修改原因。
  - 复杂需求和底层变更需要测试方案评审，要有全面的冒烟测试。
  - 分支合并，测试人员要根据diff明确测试方案和范围，不能直接信任合并过来的代码，即使它测试过，因为可能有兼容性问题。
  - 生命周期较长的分支，每次改动合入前，都要做方案评审。

#### 4、预发未充分测试

- 问题：等到快到deadline才将本地代码提交，这时候才在预发环境发现欠考虑的架构问题和重大bug，已来不及修改或测试。
- 最佳实践：
  - 频繁提交比较小的代码块。
  - 尽早进入预发环境进行测试，提前发现问题。
  - 构建失败立即通知开发，优先解决，反馈的信息要明确，避免被垃圾信息淹没，使人员忽略这些消息。

#### 5、无回滚方案

- 问题：未考虑变更后的回滚方案，会导致变更故障后，引起新的问题或恢复时间过长。需要研发对变更考虑周全，避免思考盲区。
- 解决方案：
  - 有变更就要有回滚方案，回滚方案要充分考虑可能失败的场景。
  - 通过冗余机制，如切流、蓝绿发布，让变更范围可控。
  - 回滚要考虑好直接回滚，还是需要配置项、数据库等一起回滚。

#### 6、回滚方案未验证

- 问题：回滚未验证会导致回滚失败、问题升级，引起更大的故障。
- 解决方案：
  - 回滚方案应纳入方案评审中，并对回滚做一次预演，保证回滚正确性。
  - 研发应熟悉发布系统；发布系统有改动应把回滚也纳入测试范畴。

#### 7、未评估影响范围

- 问题：每个细小的变更都应该考虑对使用方的影响，如果没有梳理清楚变更影响范围，也没有全面的回归测试，不仅对业务、用户造成损失，也损害了使用方对自己的**信赖和技术口碑**。
- 解决方案：
  - 做好代码沉淀，记录依赖方，对外提供API增加埋点数据，变更心里有数。
  - 变更做充分评估并准备回归验证，对重要依赖方提前通知。
  - 准备充足回归用例做测试，上线后灰度验证。

### 四、变更域

变更是风险的源头，很大比例故障都是因为变更导致的。变更要遵循“可观测、可灰度、可回滚”。

#### 1、变更没有记录

- 问题：上下游存在变更，影响到自身系统时，需要能够快速找到变更记录，并做回滚。
- 解决方案：所有的变更必须变更系统记录，可以为变更质量控制系统提供数据支撑和管控支撑。

#### 2、变更不可管控

- 结论：变更需要有管控；管控提供封网、熔断等变更操作，支持基础设施、中间件、应用不同层次的变更。

#### 3、变更数据没有格式化

- 问题：初期时，会有很多发布平台，如DB类、配置类、中间件类，它们各自为政，数据不同也不通，很难形成统一的存储和管理，对变更数据的利用也很低。
- 解决方案：对变更抽象，推进变更数据结构统一化。抽象示例：系统名称、系统单号、执行人、变更标题、变更类型、计划开始时间、计划截止时间、变更资源类型、具体操作对象列表、发布环境、变更详情查看地址、额外信息。

#### 4、变更封不封网

- 结论：需要封网。封网是禁止在指定时段、指定业务发布变更。在重要时期封网能够减少变更发生风险，虽然会导致变更堆积、发布延后，但和重要时期发生故障比起来，要轻微的多。
- 解决方案：封网可以逐步精细，按IDC、业务线、时间段、应用等维度封网。

#### 5、变更系统自身能力缺失

- 问题：变更系统一般给内部人员使用，UV不高，设计上一般是控制台+DB方式，设计较随意，当发展起来后容易出问题。
- 解决方案：涉及切流、断网、故障注入、演练执行、故障恢复等变更操作，必须涉及容灾设计。原则是：
  - 尽量少的依赖
  - 依赖多机房能力和多机房部署
  - 控制台设计多机房部署，必须具备自主选主能力

#### 6、版本不一致

- 问题：由于网络抖动、人为疏忽，没有将变更推到所有机器上，导致版本不一致，从而出现未知问题。
- 解决方案：变更后需要对变更进行check，使用巡检扫描版本一致性，能够及时发现问题并告警。

#### 7、业务高峰期进行数据库变更

- 问题：在业务高峰期变更表结构，引起数据库RT飙升，引起故障。
- 基本原理：表结构变更首先按照新的表结构建空表，将原表的全量数据拷贝到新表中，完成增量追平后Rename新旧表名。整个过程大量的数据拷贝会消耗IO资源，发生表结构锁竞争，SQL处理能力下降。
- 解决方案：日常关注表数据量大小，定期对历史数据归档，确保表在合理范围。表结构变更放在业务低峰期执行。

#### 8、网络变更质量

- 问题：由于涉及到不同厂商、型号的网络设备、配置命令，不同环境的网络变更不统一，缺少标准、自动的网络变更系统。
- 解决方案：
  - 具备变更需求评审。转换需求，并同业务、厂商、运维一同Review合理性；
  - 具备变更前评审体系。识别所有可能风险，提前预判，尤其是路由、流量转发这块；
  - 具备变更过程质量控制。对每个变更子步骤做好风险评估和预判，严格执行，结果检查；
  - 具备变更后review，检查结果是否符合预期。


### 五、编码域

编码是系统开发的核心工作。需求规格书和设计文档可能会过时，但源码总是最新的。所以高质量的源码是必须的。

本章从变量、集合、控制结构、并发、防御编程、工程设计举例一些常见问题。

#### 1、魔法值的使用

- 示例：缓存中的key是有"Id#provider_"+userId，一旦前缀在别处误拼写，就会引起故障。
- 最佳实践：使用static final或enum定义常量，便于维护更新和理解。

#### 2、比较包装类的对象值

- 示例：Integer的-128至127是缓存的，可以直接==判断，除此之外都是堆上new出来的，无法使用==判断。

- 最佳实践：所有包装类都走equals比较

#### 3、POJO类属性

- 示例：Bean中同时存在getXX和isXX，会导致其他框架通过反射读取Bean属性时，随意使用到get或is其中一个方法，导致最终结果不符合预期。
- 最佳实践：禁止Bean中同时存在getXX和isXX，XX为相同名称

#### 4、添加集合元素

- 示例：使用集合时，要注意，如果是方法返回的集合对象，要确保该集合不是内部类实现的。如：Collections.emptyList()，否则一旦调用就会报错。
- 最佳实践：留意方法返回的集合是不是内部类。如Arrays.asList、Map.keySet/values/entrySet、Collections.emptyList/singletonList，这些都是内部类，不支持add/remove操作。

#### 5、集合遍历时的修改

- 示例：集合遍历时，执行add/remove/update等方法时，可能会抛出CME异常
- 最佳实践：集合遍历对元素做操作，可使用Iterator，如果是并发使用Iterator则还要加锁。

#### 6、集合排序异常

- 示例：jdk7之前Collections.sort使用的是mergeSort归并排序，jdk7及以上使用的是TimSort归并+插入，后者如果不按照Comparator规范重写方法，则可能导致IAE异常。
- 最佳实践：严格按照Comparator规范重写方法，保证小于返回-1，等于返回0，大于返回1。

#### 7、程序流程控制

- 示例：switch没有控制好Case中止，会导致非预期逻辑。高并发下使用==0判断库存，可能导致超卖。
- 最佳实践：switch一定要有default分支，中止Case；要知晓switch使用return是返回方法体，和break是不同的；高并发下建议使用<=0判断库存不足。

#### 8、高并发下的单例对象

- 最佳实践：getInstance要考虑线程安全。

#### 9、高并发下的ThreadLocal

- 示例：ThreadLocal不规范使用，容易造成内存泄漏，或数据混乱。
- 最佳实践：必须回收自定义的ThreadLocal变量，尤其是static，建议放在try-finally中回收。另外要特别注意线程池对ThreadLocal的使用，一定要清理ThreadLocal，不然及其容易数据混乱。

#### 10、异常捕获处理

- 示例：异常没有合理处理，或压根没有处理，导致后续逻辑不符合设计预期，影响核心功能。另外有些二方库抛出的Error异常，catch Exception无法捕获，从而一直抛到最外层，影响核心业务。
- 最佳实践：
  - 异常必须处理。最外层调用者必须将异常转换为用户理解的内容
  - 调用RPC、二方包、动态生成类时，必须使用Throwable捕获。

#### 11、高并发下的HashMap

- 示例：多线程使用HashMap时，当触发resize时，会导致循环链表，出现死循环。
- 最佳实践：并发调用时，使用ConcurrentHashMap。

#### 12、日志的使用

- 示例：高频输出日志，会产生很多对象，触发YGC甚至FullGC，另外写磁盘加剧性能问题，导致日志线程阻塞，影响系统整体运行。
  - 使用log4j的patternLayout打印跨网络传输异常栈（RPC），远端找不到本地类时，会采用双亲委派加载，频繁加载会出现线程阻塞。
- 最佳实践：认真评估日志输出的目的和必要性，只输出关键内容，并做好降级开关。日志框架建议使用logback。

#### 13、HTTP连接管理

- 示例：http连接使用后没有正常关闭，状态处于CLOSE_WAIT无法被回收，导致连接数超过系统上限（65535），无法再创建新的连接。
- 最佳实践：使用HTTP需要关闭处理，高并发场景使用连接池，并额外通过定时线程清理无效连接；使用HTTP长连接必须配置keepalive timeout。

#### 14、参数检查

- 示例：lise长度未限制，或json数据未校验，或未判空等问题，导致数据库更新失败、解析失败、异常报错等
- 最佳实践：不要相信用户或依赖系统返回的数据，所有输入和返回值都要做数据校验，安全处理。必要的校验如下：
  - 参数合法性：类型、长度、数量
  - 慎用强制转换
  - 使用安全的数据类型。如Long代替Integer
  - 数据检查和异常标准化。通过Filter或其他模块处理
  - 对list参数做合理长度限制
  - 对json等类似参数解析，做好容错处理
  - 对参数做好空保护。

#### 15、接口签名

- 示例：接口签名可以被上游应用识别调用方，便于针对性限流、安全处理等；签名也保障业务传参的完整性（签名中包含参数字段和方法、时间）。然而签名如果传入方式或签名算法不合理，将无法达到上述两种目的。
  - 签名放在HTTP body中，需要先解析body，不便于安全识别；另外加签和解签也相对繁琐。
- 最佳实践：身份识别签名应放在query参数中(GET请求)或HTTP header里，并作为独立字段，让识别更快速和简单，不要放在HTTP body中。

#### 16、不合理的参数配置

17、版本依赖问题

18、字符防乱码

19、不限制集合的大小

### 六、测试域

1、测试链路不完全

2、测试引发的性能问题

3、测试引发的数据污染

4、线上引流测试

5、建立测试基线

6、项目重构测试保障

### 七、监控告警域

1、监控误报成本浪费

2、指标采集不标准

3、基础设施产品未关注业务可用性

4、大面积coredump监控

5、监控失效

6、监控配置不合理

7、关键报警无人处理

8、缺乏分维度大盘

9、变更不关注业务监控

### 八、故障处理域

1、应急原则

2、应急启动

3、应急组织

4、跨域协作

5、应急指挥

6、故障复盘

7、快速恢复

## 读后感

- 始于2022年5月1日，计划于应终于2022年5月3日晚之前读完，即花2天的时间来学习。

- 阅读本书的目的：了解应用架构的失败设计理念和实践原则，从而提升自己在系统设计上的眼界。