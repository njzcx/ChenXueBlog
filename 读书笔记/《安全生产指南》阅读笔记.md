## 面向失败设计 理论篇	

### 一、面向失败设计概述

​	故障注定会发生，随着时间的流逝一切软硬件运行终将失败，而保障业务持续可用，取决于我们愿意投入多少精力和成本，沉淀多少能力，是否预料到不可知的情况。

​	因为失败无处不在，尤其当系统架构复杂性增高，随之而来的失败概率也提高了。

> 一个优秀的架构师，通常是一个悲观主义、现实主义者。

**失败原因可能有**：

- 硬件问题：硬件会老化、会被外力损坏、有残次品率
- 软件bug：没有人可以保证程序没有bug，再者现在升级迭代比较快，bug不可避免
- 配置变更错误：运维变更考虑不全而疏漏
- 系统恶化：长时间工作的程序不再可用。如自增变量越界，缓存、磁盘、数据库空间不足
- 超预期流量：双十一或健康码
- 外部攻击：流量或安全攻击。如：DDOS和CC攻击
- 依赖库问题：二方库、三方库可能存在漏洞
- 依赖服务问题：其他服务可能不可用、超时，这种失败导致自己服务不可能，导致持续辐射，变成大面积服务不可用。

**如何应对上述失败**：

面向失败设计，将失败当成系统设计的一部分，**能够从失败中快速恢复时面向失败的核心思想**。

- 冗余设计避免单点故障：集群多节点部署，数据库主备，存储多副本。
- 宏观多活架构：预防天灾人祸，冷备、热备、两地三中心、多活。
- 服务能力与依赖的自我保护：通过优良的设计，在任何系统破坏后，核心功能仍能够保持处于正常工作状态。如限流、降级等。
- 不可预料场景的预案：不是所有失败都可以自动恢复的，需要针对未知失败做好预案。
- 精细化监控体系：感知失败发生。监控告警、根因定位、智能预测、智能决策。
- 自动化运维管控：白屏运维，避免操作失误、决策失误。运维需要遵循灰度原则，降低失败的爆炸半径。
- 失败演练：面向失败设计做实践验证。

### 二、冗余设计避免单点故障

要解决单点故障，需要**通过冗余和隔离**实现。如：多机房、电力双线、双线网线网卡、磁盘冗余、主备数据库、集群多节点。

- 冗余可快速接入并承担已发生的故障。

- 隔离可使故障不影响现有系统的服务质量。

针对集群多节点，无状态节点扩容即可，有状态节点需要考虑：

- 状态同步：流量分配规则，如一致性Hash算法；或共享存储系统。
- 操作互斥：分布式锁；分区分割模式。
- 主节点冗余：主备部署；分布式选主。

站点架构一般采用分层解决单点故障。

单点故障的故障域局限于机房内的服务节点，当故障域扩大到机房、地域、光纤，则进入到容灾架构的领域中了。

### 三、宏观多活架构

容灾系统评估标准Share78：https://zhuanlan.zhihu.com/p/66835461

容灾系统的设计，主要受制于建设成本，分为7级。

**本章主要讲 集团多活容灾架构的思路和设计，其中包含优点和局限性。可作为竞品分析。**

### 四、服务能力与依赖的自我保护

分布式架构有很多依赖，为避免局部问题被放大成全局问题，需要以最小的粒度将失败隔离开，以保证系统稳定性。

- 尽早拦截无效请求：DDos清洗拦截同个IP同个用户的大量请求；对业务不产生实际意义的请求，如秒杀。
- 漏斗原则：越到链路后端，耗费资源越多，所以从前到后的请求量应逐级递减。如通过CDN、缓存手段。
- 隔离原则：1、下游弱依赖降级；2、下游强依赖隔离，通过线程池、信号量、并发数实现；3、异常机器可隔离；4、异常流量可隔离
- 流量错峰：匀速器来削峰填谷。
- 按需分配：provider通过QPS限流来提供服务，此外需要提供限流后的Fallback。
- 其他：失败请求用户体验；失败请求是否可以幂等

### 五、为失败准备预案

- 什么是预案
  - 预案是“知识点”和“经验教训”，在非系统预期的场景下，系统无法自己处理时的一些人工处置方案。
- 为什么要准备预案
  - 有些系统问题并没有规律可循，或没有自动化处理方式。但通过预案，可在关键时刻，将处理这些问题
  - 有前提条件需要预案：比如系统支持100个并发，那超过100个并发怎么办？
  - 需要人工干涉的步骤需要预案：确保操作正确性。
  - 故障场景需要预案
  - 业务变更需要预案
- 如何准备预案
  - 预案执行前的判断条件要清晰
  - 预案执行步骤要准确、清晰
  - 预案执行后要有校验步骤
  - 预案执行的各参与角色尽量在流程上隔离，保证每个角色至关心自己范围内的事情。
  - 预案的生命周期：梳理预案（明确需要预案的风险点）、开发预案（针对风险点设计预案）、测试预案、演练预案（定期演练保证预案的准确性，预案的准确性应该是最高的）、下线预案。

### 六、精细化监控体系

监控的三个目的：

- 故障检测：检测故障
- 性能分析：检测性能瓶颈
- 容量规划：分析容量设计是否合理

监控视角：

- 白盒监控：站在系统视角，回答“哪里出了问题”，协助定位根因。
- 黑盒：站在用户视角，回答“系统出了问题吗”，支持检测和告警。

监控类别：

- 全栈监控：包含下列所有的监控类别
- 业务监控：从业务上监控服务状况，回答“系统出了问题吗”。
- 应用监控：从应用上监控QPS、RT、成功率失败率、JVM等核心指标，回答“哪里出了问题”。
- 容器监控：从容器上监控CPU、内存、磁盘IO和容量
- 主机监控：从主机上监控CPU、内存、磁盘IO和容量、网卡、网络联通性等。

监控技术：

- Metrics：度量，收集一段时间的数据，聚合特征并以某种模型展示出来。开源工具有dropwizard、Iris、Prometheus。
- Tracing：全链路监控，基于Google Dapper论文，可在分布式架构下，实现点到点每一条链路上的监控。核心技术是通过Agent代理插桩埋点，记录TraceId和TraceLog到KV存储设备，之后统计展示。开源工具有zipkin、openTracing、jaeger。
- Logging：监控日志并采集、清洗、展示。一般基于ELK实现。

监控维度：需要站在全局架构视角去设计，例如从上到下 站点 - 机房 - 集群 - 实例 4个基本维度。

监控精度：秒、分钟级别。分别在实效性和成本上有不同的侧重。

统计方式：采用合理的统计方式，避免引入噪音，影响决策。常用的统计方式有原始值、最大/小值、总和、平均值、数量、百分比、百分位等。

智能化监控：传统的人工设置阈值没办法应对大规模分布式应用的监控要求。而通过统计学和机器算法在不同场景下使用不同的算法，实现监控智能化，是未来的发展趋势。如线性回归、加权移动平均、指数平滑算法、聚类算法、深度学习模型等。

### 七、自动化运维管控

运维体系演进基本遵循：人肉时代 -> 工具时代 -> 平台/系统时代 -> 数据智能时代 

自动化体系演进：操作自动化 -> 场景自动化（结合上下文和外部环境） -> 数据驱动自动化（基于采集数据和算法模型）

运维常见日常5个问题：人工配置规则失误；灰度升级与回滚；消息队列丢数感知与补偿；同主机中VM相互干扰导致的CPU飙升；告警下如何熔断止血

对应解决方案（5个特征）：

- 监控指标精细透明：分为依赖基础设施指标和业务指标，前者如宿主机、中间件、存储、数据库等，后者如成交率、退款率等
- 问题快速发现
- 预案自动执行
- 变更安全可控
- 答疑高效支持

自动化运维平台设计核心思路：把一切变更标准化、流程化、自助化、自动化，降低人工干预程度，提升操作效率。一切以客户需求出发，持续抽象和迭代。

运维平台流程：监控数据采集 -> 异常识别 -> 根因分析 -> 异常/变更处理 -> 安全机制(灰度、回滚、权限控制)

运维管理平台本质上是解决安全和效率问题，衡量运维平台的好坏，可以通过：

- 故障后问题的定位、修复时间是否缩减。
- 日常答疑支持上的投入程度是否降低
- 开发人员工作打断频率是否下降，在重复性事务上的投入比重是否减少，关键产出是否提升。

### 八、失败演练

故障场景下的问题：

- 故障预案是否真实有效？
- 故障恢复工具是否实现了容灾？
- 处理故障人员是否熟练？
- 沟通机制是否疏漏？
- 容灾措施影响是否会辐射上一层？

故障演练定义：通过平台化方式沉淀故障场景，以可控成本在线上故障重放，通过持续性的演练和回归方式暴露问题，提升问题的响应与修复能力，缩短故障恢复时长。

故障演练的工具和策略：

- 引入混沌工程，建立面向失败设计和拥抱失败的文化，通过实验变量去暴露问题。
- 实施混沌工程，定义一个清晰可衡量的目标。如故障覆盖率，SLA等
- 推广混沌工程，贴近生产环境实验，结果越真实。准备大于执行，重点关注实验方案的评估，明确系统稳态和终止条件。规模化推广。

实践策略：

- 建立一个围绕稳态行为的假说：从有预期的故障注入和无预期的突袭演练，观察可测量的输出和短时间的测量结果，验证系统是否工作。
- 多样化真实世界的事件：从网络、存储、虚拟化VM、操作系统、中间件、数据库、负载均衡、应用等整体考虑故障的完备性。
- 在生产环境运行实验：仿真环境、沙箱环境，在生产环境控制好爆炸半径进行演练更容易暴露问题。
- 持续自动化运行实验：一整套演练流程包含：计划、执行、观察、记录、还原、分析。能够自动化编排和持续运行是基础。
- 最小爆炸半径：注入隔离、流量隔离、数据隔离。

## 面向失败设计 实践案例篇

### 一、架构域

#### 1、单点故障

- 无论系统其他部分做得再完备，只要系统中存在单点问题并发生故障，将引起整体故障。
- 遵循高可用架构设计，做好软硬件冗余架构，在故障出现时能够切换。
- 定期对单点进行巡检、迭代优化，通过故障注入发现系统中的潜在单点故障。要有系统化思考能力，即见树木又见森林。

#### 2、负载不均

- 负载不均会使某个节点长期处于过载状态，当流量到临界点或突增，可能导致集群服务节点全部打爆导致不可用（葫芦娃救爷爷，一个一个送）；存储热点也属于负载不均的一种表现。
- 观察负载是否均衡，不均衡时需要水平扩容或重新设计分配方案。
- 热点数据问题可以通过分桶或热点缓存来规避，尽量把请求分散到多个节点上。
- 要注意负载均衡设备自身是否会成为瓶颈

#### 3、有状态

- 无状态的单次请求不依赖其他请求，服务器本身不存储任何信息，比如HTTP协议。有状态会依赖其他请求或缓存信息，比如Session，有状态服务由于状态绑定比较难扩展。
- 在分布式设计中，考虑到扩展性，应尽量规避有状态设计。
- 不要将IP和磁盘路径写死在代码或配置中。
- 本地磁盘只写日志，不存储业务相关数据。

#### 4、事后监控

- “为什么我们没有提早发现问题”，这说明监控项是有遗漏的，对应的Action是补监控。但实际上在系统设计时就应该考虑监控埋点，设计监控项来回答“出问题了吗”、“哪里出问题了”、“具体什么问题”。
- 系统设计之初就应该设计好监控，而不是事后再补。
- 监控覆盖指标可分为OS级别（CPU、Mem、Load、磁盘、网络）、应用容器级别（Nginx、Jvm、Tomcat指标）、业务级别（QPS、TPS、RT、成功率、转化率等）。
- 指标要有环比视图，和告警能力。

#### 5、不可回滚

- 任何升级或变更都会带来系统性风险，所以应该具备回滚能力。
- 变更和回滚尽量做的自动化，当回滚不便做自动化时，可以通过“回滚脚本”或“回滚方案”支持。
- 不要盲目自信，升级变更一定要有回滚方案，而且一定要在上线前review。

#### 6、不可降级

- 当系统访问量激增、性能下降，非核心服务影响到核心服务的使用时，为保证主流程可用，需要对非核心服务进行降级。核心思想是通过对非核心界面和服务有策略不做处理，来释放服务器资源保证核心服务正常运作。
- 常见的降级思路：1）通过开关统一降级；2）多个开关组合形成预案，按预案降级；3）限流；4）熔断；
- 系统设计时要甄别强弱依赖，弱依赖不可用时不能导致系统崩溃；开关、预案需要沉淀下来，并适时做演练。

#### 7、缺乏隔离

- 当系统出现故障时，隔离能够将故障的影响控制在一定范围，限制爆炸半径。另一方面也可以减少资源竞争。
- 常见隔离方案：
  - 动静资源隔离（CDN+Server）
  - 线程隔离：不同重要性任务放在不同线程池中
  - 进程隔离：应用拆分成多个重要性不同的服务，分开部署
  - 集群隔离：将同一个服务部署多个集群，集群分组提供不同的SLA
  - 机房隔离：服务多机房部署，机房内优先调用
  - 租户隔离：租户之间隔离，互相不影响。
- 系统设计时要考虑好核心服务和非核心服务，从上述隔离方案中逐层对服务拆分，保证其隔离性，承诺不同的SLA。

#### 8、缺乏冗余备份

- 工程领域、航天领域、计算机领域都有应用“冗余备份”模型，对核心系统的依赖做多份冗余。
- 是否要做冗余备份，要结合可用性以及成本来看是否值得。
- 对应用外部依赖进行整理，依赖最小化，重要依赖组件增加冗余备份机制。

### 二、设计域

- 良好的设计需要趋同大家共同的理解，如有特殊性，需要在代码里做相关说明，避免很多低级故障，避免给同事挖坑。
- 设计师需要具备事物抽象能力，解耦能力；有需要从细节把控，做到大处着眼，小处着手。
- 软件设计领域有很多设计思想：面向过程、面向对象、面向服务、面向消息、面向失败，都是在不同阶段解决不同问题，没有好坏区分。
- 编码设计：命名要见名知意；注意解偶，特别是开闭原则
- 分布式设计：关注一致性，避免脑裂问题。
- 缓存设计：观察命中率，避免无效查询，打穿缓存。
- 容量设计：明确容量上限，做好保护措施。
- 依赖设计：明确强弱依赖，做好弱依赖旁路化和降级和强依赖冗余
- 日志设计：日志异步输出，避免无效日志。日志占用空间可控。
- 监控设计：确定并监控关键指标，具备报警功能，但又不要过多无效报警。
- 权限设计：够用原则，避免大权限、大账号。
- 表结构设计：分库分表关注热点数据问题；关注表数据量大小

#### 1、滥用日志策略

- 建议采用异步日志机制。日志一般存储在本地磁盘，采用异步日志，可保证业务流程不受磁盘故障影响，让写日志在旁路工作。
- 采用异步刷盘策略。将日志放入队列中，由刷盘线程定期处理，设计较为宽松的丢弃策略，避免高频日志频繁刷盘，使IO成为瓶颈，进而造成日志堆积影响内存和GC。
- 以固定文件大小保存淘汰文件，保护磁盘容量。

#### 2、缺乏自我保护

- 任何系统都有处理能力上限，所以系统自我保护很重要，系统调用其他系统时，要认为它是不靠谱的，设计一套自己的容错方案做依赖管理。
- 例如：突发的大并发请求服务端；发生异常大量的错误堆栈打印，加剧写磁盘压力。
- 通过TCP滑动窗口或拥塞控制机制缓解；通过限流组件预防突发流量；对不稳定的或超时的依赖调用自动熔断；限流后的处理逻辑尽量简单快速，避免二次伤害。

#### 3、缓存设计不当

- 缓存和数据库不一致；命不中数据导致击穿；热点数据导致雪崩。
- 通过缓存空值key或比较大的布隆过滤器，避免缓存击穿。
- 通过一致性hash进行热点散列。
- 缓存层只在数据层和服务层之间，避免嵌套调用。
- 调用接口异常，错误数据不放入缓存。
- 恰到好处的缓存策略。

#### 4、容量评估不准

- 设计系统时，需要根据压测数据，设置资源值和流控阈值，包括不限于流控QPS、连接池、上下游关联服务性能。
- 提升性能敏感性。做任何变更前，要清楚会不会带来性能影响。

#### 5、耦合过重

- 高耦合带来的问题是牵一发而动全身，不易维护、演进。
- 单个应用设计，要考虑单模块失效是否会影响整体服务。可以从代码层面、进程层面、机器、应用层面解偶，降低整体耦合。

#### 6、滥用同步

- 同步、异步没有优劣之分，要看应用在什么样的实际场景。同步优势在于开发和调试简单，交互即时反馈，但链路过长，可用性会下降。异步优势在于解偶依赖，降低链路故障的影响，但代码和调试会有一定的复杂性。
- 系统设计中，系统间交互、系统内模块交互，尽量采用异步通信。
- 独立任务、耗时任务，应该采用异步，减少阻塞和串联失败的可能性。
- 大流量服务打印日志，建议采用过AsyncAppender实现，并配合合理的buffer以及丢弃策略。

#### 7、非幂等

- 幂等：同一操作多次请求，结果必须是一致的，**不会因为多次点击或提交而产生副作用**。例如订单已支付，服务处理成功，但返回客户端失败了，再次支付应该不重复扣款。
- 同一次事务多次重复操作，确保操作唯一性和事务结果唯一性，防止重复提交。
- 同一任务多次重试，确保重试对结果无影响。

#### 8、敏感信息泄漏

- 客户信息展示注意权限和脱敏
- 日志、存储的敏感数据通过二方库脱敏

#### 9、对失败考虑不充分

- 大部分故障都是由于基础设施、三方依赖、软负载、配置中心、中间件异常导致的。
- 系统调用其他服务时，要认为他们可能会失败，而设计一套自己的方案，避免失败影响核心业务。包括不限于三方依赖容灾设计、服务熔断设计、单Region高可用、两地三中心。

#### 10、数据库索引不合理

- 唯一索引中，字段如果为NULL，那么该唯一性将会被打破（BDB引擎除外）。
- 建议：
  - 无论表大小，都建议加上索引，避免全表扫描。优先原则：单索引 > 联合索引，窄索引 > 宽索引
  - 选择区分度高的字段建立索引
  - 多字段and或多表join，注意满足最左前缀原则，区分度高的字段放在前面。
  - 尽量使用覆盖索引、前缀索引【不知道啥意思】
  - order/group by则需要排列方向一致，才能利用索引。
  - 唯一索引的字段要有非空限制或默认值。
  - 索引调整一定要确保drop+add是通过一条alter执行的

#### 11、数据库表结构设计不合理

- 主键设计要避免varchar类型，因为会加大索引负担，影响BP命中率、查询效率、插入效率。建议采用bigint类型的自增字段。
- 分库分表建议采用区分度高的唯一键，避免写入数据倾斜，查询时广播到所有节点的问题。

#### 12、误用数据库limit查询

- limit一般用于翻页查询、批量导数据控制批次。问题是：limit 1000,10会先读取1010行再扔掉前1000行，效率差，如果没有索引那么效率更差。
- 解决办法：limit和order by组合，将排序字段加入索引列；用主键代替限定查询范围，例如select id,xx from table where id > min(id) and id < max(id) and order by id limit 50，取最大的id作为下次查询的minId，不断重复查询。

#### 13、小表随意truncate

- truncate和delete table都可以清空表。
  - delete操作会记录每行的redo、undo、binlog日志，且执行后表空间不释放，只是page为空页；
  - truncate是drop table + create table的组合，会释放表空间，事务日志只记录页释放，所以数据无法恢复。
- truncate性能优于delete，但truncate导致数据库抖动，可能造成数据库暂时不可用状态，每次使用truncate需要sleep一段时间，要小心使用。具体原因与buffer pool相关，删除表时会对buffer pool加全局锁。

#### 14、一次性批量delete数据

- delete大量数据，会导致导致数据库性能、稳定性变差；update同理。
  - 产生大量binlog，master-slave同步延迟会增高，当master出现故障发生切换，会导致切换延迟大，且数据有损量也大。
  - delete数据量太大，undo log无法很快被purge，影响新会话的select查询效率。
- 建议delete、update操作按批次拆分执行，每个批次执行一小部分数据，批次间sleep进行等待，降低对数据库的影响。

#### 15、对异常状态进行兜底设计

- 平台应该信任并忠于用户的请求，但人都会犯错，对于一些明显异常的用户操作，不加阻拦很容易出现重大故障。例如删除生产环境上的所有的LVS VIP。
- 核心系统上要设计一些极端情况的兜底逻辑，不给用户犯错的机会。

#### 16、对重要数据备份

- 数据备份时所有系统都要做的一件事，当出现重大故障时可以快速恢复。例如用户删除生产环境上的所有的LVS VIP，如果有快照备份数据，就可以在最短时间恢复。

### 三、发布域

安全发布三板斧：可灰度，可观测，可回滚

发布策略：

- 金丝雀发布：用单台或5%以内的机器先发布，验证正常后，在整体发布。缺点是一般通过手工验证，大规模发布的问题无法发现。
- 滚动发布：金丝雀的改进，多个批次逐步增加发布数量，每个批次都要观察。缺点是发布周期会比较长，高一致性场景不适用。
- 蓝绿发布：部署两套新旧版本同时提供服务，通过切流实现发布。需要额外冗余的机器资源支持。

#### 1、无灰度流程

- 灰度发布是发布系统不可或缺的一部分。
- 灰度最佳要求是一个环境隔离，独立监控，流量可控的安全环境，也是一个可小量变更的环境，在该环境中通过小流量独立验证。

#### 2、错误灰度方案

- 灰度发布本身要考虑是否对配置项结构、数据库表模型、API变更向前兼容和新旧版本兼容。
- 如果不能做到兼容，应确保灰度流量不要逃逸到正常机器上；写数据通过加version保证新版本的数据只有新逻辑处理。
- 灰度方案要具备可逆性，即可以增加发布比例，也可以减少发布比例。

#### 3、未经测试上线

- 主观上常见问题：觉得改动不大，研发直接绕过测试发布；搭车发布代码。
- 客观上常见问题：回归测试范围不广泛；自动化测试用例不足；回归测试不充分；只运行低级别的回归测试；只执行变更相关的回归测试。
- 最佳实践。。。

4、预发未充分测试

5、无回滚方案

6、回滚方案未验证

7、未评估影响范围

### 四、变更域

1、变更没有记录

2、变更不可管控

3、变更数据没有格式化

4、变更封不封网

5、变更系统自身能力缺失

6、版本不一致

7、业务高峰期进行数据库变更

8、网络变更质量

### 五、编码域

1、魔法值的使用

2、比较包装类的对象值

3、POJO类属性

4、添加集合元素

5、集合遍历时的修改

6、集合排序异常

7、程序流程控制

8、高并发下的单例对象

9、高并发下的ThreadLocal

10、异常捕获处理

11、高并发下的HashMap

12、日志的使用

13、HTTP连接管理

14、参数检查

15、接口签名

16、不合理的参数配置

17、版本依赖问题

18、字符防乱码

19、不限制集合的大小

### 六、测试域

1、测试链路不完全

2、测试引发的性能问题

3、测试引发的数据污染

4、线上引流测试

5、建立测试基线

6、项目重构测试保障

### 七、监控告警域

1、监控误报成本浪费

2、指标采集不标准

3、基础设施产品未关注业务可用性

4、大面积coredump监控

5、监控失效

6、监控配置不合理

7、关键报警无人处理

8、缺乏分维度大盘

9、变更不关注业务监控

### 八、故障处理域

1、应急原则

2、应急启动

3、应急组织

4、跨域协作

5、应急指挥

6、故障复盘

7、快速恢复

## 读后感

- 始于2022年5月1日，计划于应终于2022年5月3日晚之前读完，即花2天的时间来学习。

- 阅读本书的目的：了解应用架构的失败设计理念和实践原则，从而提升自己在系统设计上的眼界。